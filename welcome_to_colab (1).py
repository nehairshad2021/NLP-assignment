# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""



from google.colab import drive
drive.mount('/content/drive')

'/content/drive/MyDrive/seeking2020.npy'

import nltk
from nltk.corpus import reuters
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
import spacy
import string
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import OneHotEncoder
import numpy as np

# Load Reuters dataset or use any other corpus
nltk.download("reuters")
nltk.download("stopwords")
nltk.download("punkt")

corpus = [" ".join(reuters.words(fileid)) for fileid in reuters.fileids()]

stop_words = set(stopwords.words("english"))
punctuation = string.punctuation

def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text.lower())
    # Remove punctuation and stop words
    tokens = [word for word in tokens if word.isalpha() and word not in stop_words and word not in punctuation]
    return tokens

processed_corpus = [preprocess_text(doc) for doc in corpus]

from nltk.stem import PorterStemmer, WordNetLemmatizer

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()
nltk.download("wordnet")

def stem_and_lemmatize(tokens):
    stemmed = [stemmer.stem(word) for word in tokens]
    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]
    return stemmed, lemmatized

processed_corpus_stemmed = [stem_and_lemmatize(doc)[0] for doc in processed_corpus]
processed_corpus_lemmatized = [stem_and_lemmatize(doc)[1] for doc in processed_corpus]

# Word2Vec model training
model = Word2Vec(sentences=processed_corpus, vector_size=100, window=5, min_count=1, workers=4)

# Example: Get vector for a word
word_vector = model.wv['economy']
print(word_vector)

vectorizer = CountVectorizer()
bow_matrix = vectorizer.fit_transform([" ".join(doc) for doc in processed_corpus])

# Example: Get BoW encoding for first document
print(bow_matrix.toarray()[0])

encoder = OneHotEncoder()
# Flatten corpus for encoding
flattened_corpus = np.array([word for doc in processed_corpus for word in doc]).reshape(-1, 1)
onehot_encoded = encoder.fit_transform(flattened_corpus)

# Example: Get One-Hot encoding for a word
print(onehot_encoded.toarray()[0])

nlp = spacy.load("en_core_web_sm")

def pos_tagging(doc):
    doc = nlp(" ".join(doc))
    return [(token.text, token.pos_) for token in doc]

# Example: POS tagging for the first document
pos_tags = pos_tagging(processed_corpus[0])
print(pos_tags)

'/content/drive/MyDrive/seeking2020.npy'

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
import spacy

!pip install vaderSentiment # Install the vaderSentiment library

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer # Import SentimentIntensityAnalyzer from vaderSentiment
from textblob import TextBlob # Import TextBlob
import spacy # Import spacy

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
import spacy

text = """
Apple Inc. is looking at buying U.K. startup for $1 billion. This move has generated excitement among investors, who believe it will greatly benefit Apple in the long run. However, some analysts are skeptical and advise caution.
"""
# Initialize VADER sentiment analyzer
vader_analyzer = SentimentIntensityAnalyzer()

# Perform sentiment analysis
vader_sentiment = vader_analyzer.polarity_scores(text)
print("VADER Sentiment Scores:", vader_sentiment)
# Perform sentiment analysis with TextBlob
blob = TextBlob(text)
print("TextBlob Sentiment Polarity:", blob.sentiment.polarity)
print("TextBlob Sentiment Subjectivity:", blob.sentiment.subjectivity)
# Perform sentiment analysis with TextBlob
blob = TextBlob(text)
print("TextBlob Sentiment Polarity:", blob.sentiment.polarity)
print("TextBlob Sentiment Subjectivity:", blob.sentiment.subjectivity)